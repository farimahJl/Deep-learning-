{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "joSo-co4o59g"
   },
   "source": [
    "# Medical MNIST dataset\n",
    "\n",
    "The *Medical MNIST dataset* has been selected for this project. This dataset contains 6 classes of medical images, and each class has approximately 10k samples. All the images are in the size of 64 by 64. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rf2wiHe6pCNp"
   },
   "source": [
    "**Data Exploration - Reading the dataset and extracting features**\n",
    "\n",
    "Each folder in the archive directory contains sample images in each class. So, the name of folders are considered as *Label Names* and the number of images in each class (folder) is reported. Also, some examples of the dataset have been showed.\n",
    "\n",
    "All images (after converting from bgr to grayscale) have the shape of [64,64,1]. So, totaly, each data sample (image) has 64 × 64 × 1 = 4096 features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "tFnUfbf5wEHH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "hlRbfh31pF5r",
    "outputId": "1054a380-b787-4d2c-ee5f-4359790b4b69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Samples (images) in Class  AbdomenCT =  10000\n",
      "No. of Samples (images) in Class  BreastMRI =  8954\n",
      "No. of Samples (images) in Class  ChestCT =  10000\n",
      "No. of Samples (images) in Class  CXR =  10000\n",
      "No. of Samples (images) in Class  Hand =  10000\n",
      "No. of Samples (images) in Class  HeadCT =  10000\n",
      "Total Data Samples (number of images):  58954\n",
      "Number of features in each sample =  4096\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "k=0;           # counter for data samples\n",
    "Dataset = [];  # the dataset that will be used in classical models (non deep)\n",
    "imgdb = [];    # the dataset that will be used in Neural Network models (fully-connected and CNN)\n",
    "Label = [];    # Label of images = folder name\n",
    "fig = plt.figure()\n",
    "im_counter=1;\n",
    "for dir1 in os.listdir('archive'):\n",
    "  class_counter=0;\n",
    "  for file in os.listdir(os.path.join('archive', dir1)):\n",
    "    image_path= os.path.join('archive', dir1,  file)\n",
    "    im =np.asarray(Image.open(image_path))\n",
    "\n",
    "    V = np.reshape(im, (1,np.size(im)));\n",
    "    Dataset.append(V[0])                # appending feature vector of current image to the Dataset (for non deeps)\n",
    "    imgdb.append(im)                    # appending feature vector of current image to the imgdb (for NNs)\n",
    "    Label.append(dir1)                  # appending the folder name as the label of the current image\n",
    "    k = k+1;\n",
    "    class_counter = class_counter + 1\n",
    "  # show examples of images\n",
    "  ax = fig.add_subplot(1, 6, im_counter)\n",
    "  im_counter = im_counter + 1\n",
    "  imgplot = plt.imshow(im, cmap='gray')\n",
    "  print(\"No. of Samples (images) in Class \", dir1, \"= \" , class_counter)\n",
    "print(\"Total Data Samples (number of images): \", k)\n",
    "print(\"Number of features in each sample = \", len(V[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jk9_qOxc3x2z"
   },
   "source": [
    "**Training Classical Models: Naive Bayes - K Nearest Neighbors - Decision tree**\n",
    "\n",
    "Three different classical ML models have been selected for medical image classification: Naiive Bayes which is a fast probablistic model, Decision Tree, and Logisitc Regression. \n",
    "\n",
    "K Nearest neighbors also may provide high accuracy, but it's not practical as the dataset is large and prediciting will be very slow. So, it has not selected. \n",
    "\n",
    "Support vector machine (SVM) is another interesting alternative. But, as we will use different neural networks in next parts, I tried to choose simple algorithms and check their prformance against advanced models. \n",
    "\n",
    "\n",
    "First we split the dataset into training (70%) and test (30%) subsets (holdout method). k-fold cross validation strategy has not choosen, because the dataset is large enough, and 30% of that is about 30,000 samples which has a good diversity for evaluation. Also, holdout method is far faster than k-fold cross validation. \n",
    "\n",
    "Then we train three mentioned ML models based on training subset, and then provide the labels for both training and test subsets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "wxUiV_e0wEHN"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "6FzcGNpA30Pb"
   },
   "outputs": [],
   "source": [
    "# split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(Dataset, Label, test_size=0.3, random_state=0)\n",
    "\n",
    "# 1 - Naive Bayes\n",
    "gnb = GaussianNB();\n",
    "model_gnb = gnb.fit(X_train, y_train)\n",
    "y_predict_gnb = model_gnb.predict(X_test)\n",
    "y_predict_gnb_training = model_gnb.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "1DgWv9u8FCQ7"
   },
   "outputs": [],
   "source": [
    "# 2 - Decision Tree\n",
    "tr = tree.DecisionTreeClassifier()\n",
    "model_tree = tr.fit(X_train, y_train)\n",
    "y_predict_tree = model_tree.predict(X_test)\n",
    "y_predict_tree_training = model_tree.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Met4EdAVFDkg",
    "outputId": "9e885056-1995-4275-ead2-b3f374268636"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Farimah\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# 3 - Logistic Regression\n",
    "lr = LogisticRegression(random_state=0)\n",
    "model_lr = lr.fit(X_train, y_train)\n",
    "y_predict_lr = model_lr.predict(X_test)\n",
    "y_predict_lr_training = model_lr.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8gM__AlSJ4s"
   },
   "source": [
    "**Evaluation of classical models**\n",
    "\n",
    "Using previous provided labels for training and test samples, in this section, the evaluation criteria for the results have been calculated and reported. \n",
    "\n",
    "For all the three models, the values of Accuracy, Precision, Recall, and F-score, have been calculated for both training and test subsets. Also, the confusion matrix for test subsets is reported for all models. It is quite a balanced dataset, so we can use accuracy as metric. However, having multiple metrics is always handy.\n",
    "\n",
    "There is a bit of overfitting as training accuracy is higher than test accuracy. The accuracy for all models is higher that 0.97, and there is a little difference between training and test results. \n",
    "\n",
    "The best model among these three models is Decision Tree with the acc. of 0.9938"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "flbbAwjkSM8B",
    "outputId": "1ec7035a-07fb-40d2-f990-00a4033af11e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naiive Bayes Results: \n",
      "confusion Matrix: \n",
      "[[2968    0   24    0    2    0]\n",
      " [   0 2626    0    0   43    0]\n",
      " [   0    0 2963    0   44    0]\n",
      " [   0    0   35 2946    0    0]\n",
      " [   0    0  131    0 2893    0]\n",
      " [   0    0    0    0  101 2911]]\n",
      ">> Test Set:\n",
      "Accuracy =  0.9785152937185504    Precision =  0.9792182981258163   Recall =  0.9785152937185504   F-Score =  0.9786921401795132\n",
      ">> Training Set:\n",
      "Accuracy =  0.9807836770300724    Precision =  0.9813026086373493   Recall =  0.9807836770300724   F-Score =  0.9809157985963062\n",
      "----------------------------------------------------------------------------\n",
      "Logistic Regression Results: \n",
      "confusion Matrix: \n",
      "[[2985    0    0    9    0    0]\n",
      " [   0 2669    0    0    0    0]\n",
      " [   3    1 2975    5   17    6]\n",
      " [   5    0    0 2976    0    0]\n",
      " [  12    0   23    5 2919   65]\n",
      " [  15    0    5   42   40 2910]]\n",
      ">> Test Set:\n",
      "Accuracy =  0.9856957087126138    Precision =  0.985680528044664   Recall =  0.9856957087126138   F-Score =  0.9856553897450294\n",
      ">> Training Set:\n",
      "Accuracy =  0.9918094361111784    Precision =  0.9918396975791289   Recall =  0.9918094361111784   F-Score =  0.9917987739981087\n",
      "----------------------------------------------------------------------------\n",
      "Decision Tree Results: \n",
      "confusion Matrix: \n",
      "[[2994    0    0    0    0    0]\n",
      " [   0 2669    0    0    0    0]\n",
      " [   0    1 2959    3   41    3]\n",
      " [   0    0    0 2977    4    0]\n",
      " [   1    2   30    1 2982    8]\n",
      " [   0    0    1    0   14 2997]]\n",
      ">> Test Set:\n",
      "Accuracy =  0.9938372816192684    Precision =  0.9938453466403766   Recall =  0.9938372816192684   F-Score =  0.9938385178591529\n",
      ">> Training Set:\n",
      "Accuracy =  1.0    Precision =  1.0   Recall =  1.0   F-Score =  1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# 1: Naiive Bayes results:\n",
    "print(\"Naiive Bayes Results: \")\n",
    "print(\"confusion Matrix: \")\n",
    "print(confusion_matrix(y_test, y_predict_gnb))\n",
    "acc_gnb = (y_test == y_predict_gnb).sum() / len(y_test)\n",
    "gnb_res = precision_recall_fscore_support(y_test, y_predict_gnb, average='weighted')\n",
    "print(\">> Test Set:\")\n",
    "print(\"Accuracy = \", acc_gnb, \"   Precision = \" , gnb_res[0], \"  Recall = \", gnb_res[1], \"  F-Score = \", gnb_res[2])\n",
    "print(\">> Training Set:\")\n",
    "acc_gnb_training = (y_train == y_predict_gnb_training).sum() / len(y_train)\n",
    "gnb_res_training = precision_recall_fscore_support(y_train, y_predict_gnb_training, average='weighted')\n",
    "print(\"Accuracy = \", acc_gnb_training, \"   Precision = \" , gnb_res_training[0], \"  Recall = \", gnb_res_training[1], \"  F-Score = \", gnb_res_training[2])\n",
    "print('----------------------------------------------------------------------------')\n",
    "\n",
    "# 2: Logistic Regression results:\n",
    "print(\"Logistic Regression Results: \")\n",
    "print(\"confusion Matrix: \")\n",
    "print(confusion_matrix(y_test, y_predict_lr))\n",
    "acc_lr = (y_test == y_predict_lr).sum() / len(y_test)\n",
    "lr_res = precision_recall_fscore_support(y_test, y_predict_lr, average='weighted')\n",
    "print(\">> Test Set:\")\n",
    "print(\"Accuracy = \", acc_lr, \"   Precision = \" , lr_res[0], \"  Recall = \", lr_res[1], \"  F-Score = \", lr_res[2])\n",
    "print(\">> Training Set:\")\n",
    "acc_lr_training = (y_train == y_predict_lr_training).sum() / len(y_train)\n",
    "lr_res_training = precision_recall_fscore_support(y_train, y_predict_lr_training, average='weighted')\n",
    "print(\"Accuracy = \", acc_lr_training, \"   Precision = \" , lr_res_training[0], \"  Recall = \", lr_res_training[1], \"  F-Score = \", lr_res_training[2])\n",
    "print('----------------------------------------------------------------------------')\n",
    "\n",
    "# 3: Tree Results\n",
    "print(\"Decision Tree Results: \")\n",
    "print(\"confusion Matrix: \")\n",
    "print(confusion_matrix(y_test, y_predict_tree))\n",
    "accuracy = (y_test == y_predict_tree).sum() / len(y_test)\n",
    "tree_res = precision_recall_fscore_support(y_test, y_predict_tree, average='weighted')\n",
    "print(\">> Test Set:\")\n",
    "print(\"Accuracy = \", accuracy, \"   Precision = \" , tree_res[0], \"  Recall = \", tree_res[1], \"  F-Score = \", tree_res[2])\n",
    "print(\">> Training Set:\")\n",
    "acc_tree_training = (y_train == y_predict_tree_training).sum() / len(y_train)\n",
    "tree_res_training = precision_recall_fscore_support(y_train, y_predict_tree_training, average='weighted')\n",
    "print(\"Accuracy = \", acc_tree_training, \"   Precision = \" , tree_res_training[0], \"  Recall = \", tree_res_training[1], \"  F-Score = \", tree_res_training[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNJhD9f3_cp2"
   },
   "source": [
    "**Preparing Dataset for Neural Networks**\n",
    "\n",
    "The labels in Keras should be in the Categorical type. But our labels are in the string type. So, in this section, we first turn string values of the labels into numbers, and then turn the numbers into categorical using *to_categorical* function. \n",
    "\n",
    "Also, we use the real shape of inputs (i.e. 64 × 64 × 1) in this section and next one.\n",
    "\n",
    "All the input images are converted from uint8 into float32 type, and normalized between 0 and 1 values. \n",
    "\n",
    "The batch size for both neural networks is set to 256 which is large enaugh to train faster, and small enaugh for preventing the out-of-memory problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "f1hPnWe3_iKz"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# turn string values to numbers 0-5\n",
    "Numerical_Label = [];\n",
    "for i in range(len(Label)):\n",
    "  if Label[i]=='AbdomenCT':\n",
    "    Numerical_Label.append(0);\n",
    "  elif Label[i]=='BreastMRI':\n",
    "    Numerical_Label.append(1);\n",
    "  elif Label[i]=='CXR':\n",
    "    Numerical_Label.append(2);\n",
    "  elif Label[i]=='ChestCT':\n",
    "    Numerical_Label.append(3);\n",
    "  elif Label[i]=='Hand':\n",
    "    Numerical_Label.append(4);\n",
    "  elif Label[i]=='HeadCT':\n",
    "    Numerical_Label.append(5);\n",
    "\n",
    "# turn numbers into categorical:\n",
    "Numerical_Label = np.array(Numerical_Label)\n",
    "imgdb = np.array(imgdb)\n",
    "output_matrix = to_categorical((Numerical_Label))\n",
    "num_classes = 6\n",
    "input_shape = (64, 64, 1)\n",
    "\n",
    "# the data, split between train (70%) and test sets (30%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(imgdb, output_matrix, test_size=0.3, random_state=0)\n",
    "\n",
    "# Normalization and unit8 to float32 conversion\n",
    "X_train = X_train.astype(\"float32\") / 255\n",
    "X_test = X_test.astype(\"float32\") / 255\n",
    "\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tn7S0pDEX5Kw"
   },
   "source": [
    "**Fully-Connected Neural Network**\n",
    " \n",
    "I tested some networks, and finally the best setting achieved as follows:\n",
    "- 2 hidden layers, each 10 neurons\n",
    "- all activation functions = 'ReLu'\n",
    "- Learning rate = 0.0001\n",
    "- optimizer = Adam\n",
    "\n",
    "It's a small model so it does not overfit quickly. Adam uses momentum and adaptive learning to learn faster.\n",
    "\n",
    "Other settings (like using tanh and sigmoid activations, or using stochastic gradient descent optimizer, or bigger learning rates) caused bigger loss in the model. Also smaller learning rates make training slower and needing more epochs for training. \n",
    "\n",
    "The results clearly shows that, this architecture could completely learn the problem, as training accuracy is very close to 100%.\n",
    "\n",
    "Also, The accuracy for test subset is bigger than decision tree result. So, The FCNN is better than classical models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LmQ8HuQeX1ho",
    "outputId": "e728e78f-974e-4ee7-f3db-b8ea9234cb23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 4096)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                40970     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 6)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 41,146\n",
      "Trainable params: 41,146\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "162/162 [==============================] - 3s 12ms/step - loss: 1.2667 - accuracy: 0.4669\n",
      "Epoch 2/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.7538 - accuracy: 0.7593\n",
      "Epoch 3/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.5134 - accuracy: 0.9130\n",
      "Epoch 4/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.3740 - accuracy: 0.9412\n",
      "Epoch 5/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.2948 - accuracy: 0.9467\n",
      "Epoch 6/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.2439 - accuracy: 0.9528\n",
      "Epoch 7/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.2106 - accuracy: 0.9559\n",
      "Epoch 8/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1858 - accuracy: 0.9607\n",
      "Epoch 9/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1678 - accuracy: 0.9656\n",
      "Epoch 10/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1540 - accuracy: 0.9689\n",
      "Epoch 11/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1428 - accuracy: 0.9716\n",
      "Epoch 12/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1337 - accuracy: 0.9732\n",
      "Epoch 13/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1260 - accuracy: 0.9746\n",
      "Epoch 14/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1193 - accuracy: 0.9762\n",
      "Epoch 15/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1136 - accuracy: 0.9773\n",
      "Epoch 16/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1081 - accuracy: 0.9785\n",
      "Epoch 17/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.1036 - accuracy: 0.9794\n",
      "Epoch 18/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0995 - accuracy: 0.9803\n",
      "Epoch 19/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0956 - accuracy: 0.9811\n",
      "Epoch 20/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0925 - accuracy: 0.9817\n",
      "Epoch 21/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0885 - accuracy: 0.9825\n",
      "Epoch 22/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0858 - accuracy: 0.9831\n",
      "Epoch 23/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0828 - accuracy: 0.9837\n",
      "Epoch 24/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0800 - accuracy: 0.9842\n",
      "Epoch 25/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0768 - accuracy: 0.9844\n",
      "Epoch 26/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0741 - accuracy: 0.9850\n",
      "Epoch 27/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0721 - accuracy: 0.9855\n",
      "Epoch 28/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0689 - accuracy: 0.9859\n",
      "Epoch 29/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0674 - accuracy: 0.9862\n",
      "Epoch 30/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0650 - accuracy: 0.9866\n",
      "Epoch 31/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0634 - accuracy: 0.9868\n",
      "Epoch 32/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0612 - accuracy: 0.9874\n",
      "Epoch 33/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0592 - accuracy: 0.9876\n",
      "Epoch 34/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0575 - accuracy: 0.9881\n",
      "Epoch 35/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0562 - accuracy: 0.9880\n",
      "Epoch 36/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0543 - accuracy: 0.9883\n",
      "Epoch 37/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0531 - accuracy: 0.9885\n",
      "Epoch 38/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0514 - accuracy: 0.9890\n",
      "Epoch 39/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0502 - accuracy: 0.9891\n",
      "Epoch 40/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0485 - accuracy: 0.9893\n",
      "Epoch 41/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0475 - accuracy: 0.9894\n",
      "Epoch 42/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0461 - accuracy: 0.9896\n",
      "Epoch 43/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0447 - accuracy: 0.9900\n",
      "Epoch 44/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0434 - accuracy: 0.9903\n",
      "Epoch 45/100\n",
      "162/162 [==============================] - 2s 12ms/step - loss: 0.0423 - accuracy: 0.9903\n",
      "Epoch 46/100\n",
      "162/162 [==============================] - 2s 12ms/step - loss: 0.0411 - accuracy: 0.9906\n",
      "Epoch 47/100\n",
      "162/162 [==============================] - 2s 12ms/step - loss: 0.0400 - accuracy: 0.9909\n",
      "Epoch 48/100\n",
      "162/162 [==============================] - 2s 12ms/step - loss: 0.0390 - accuracy: 0.9909\n",
      "Epoch 49/100\n",
      "162/162 [==============================] - 2s 13ms/step - loss: 0.0377 - accuracy: 0.9913\n",
      "Epoch 50/100\n",
      "162/162 [==============================] - 2s 12ms/step - loss: 0.0368 - accuracy: 0.9913\n",
      "Epoch 51/100\n",
      "162/162 [==============================] - 2s 12ms/step - loss: 0.0355 - accuracy: 0.9919\n",
      "Epoch 52/100\n",
      "162/162 [==============================] - 2s 12ms/step - loss: 0.0352 - accuracy: 0.9919\n",
      "Epoch 53/100\n",
      "162/162 [==============================] - 2s 12ms/step - loss: 0.0337 - accuracy: 0.9921\n",
      "Epoch 54/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0329 - accuracy: 0.9926\n",
      "Epoch 55/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0318 - accuracy: 0.9931\n",
      "Epoch 56/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0314 - accuracy: 0.9932\n",
      "Epoch 57/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0302 - accuracy: 0.9943\n",
      "Epoch 58/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0292 - accuracy: 0.9946\n",
      "Epoch 59/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0285 - accuracy: 0.9951\n",
      "Epoch 60/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0277 - accuracy: 0.9952\n",
      "Epoch 61/100\n",
      "162/162 [==============================] - 2s 12ms/step - loss: 0.0269 - accuracy: 0.9958\n",
      "Epoch 62/100\n",
      "162/162 [==============================] - 2s 13ms/step - loss: 0.0261 - accuracy: 0.9958\n",
      "Epoch 63/100\n",
      "162/162 [==============================] - 2s 12ms/step - loss: 0.0257 - accuracy: 0.9961\n",
      "Epoch 64/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0250 - accuracy: 0.9959\n",
      "Epoch 65/100\n",
      "162/162 [==============================] - 2s 12ms/step - loss: 0.0241 - accuracy: 0.9959\n",
      "Epoch 66/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0236 - accuracy: 0.9960\n",
      "Epoch 67/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0231 - accuracy: 0.9962\n",
      "Epoch 68/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0223 - accuracy: 0.9965\n",
      "Epoch 69/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0220 - accuracy: 0.9963\n",
      "Epoch 70/100\n",
      "162/162 [==============================] - 2s 13ms/step - loss: 0.0212 - accuracy: 0.9964\n",
      "Epoch 71/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0208 - accuracy: 0.9964\n",
      "Epoch 72/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0200 - accuracy: 0.9968\n",
      "Epoch 73/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0198 - accuracy: 0.9968\n",
      "Epoch 74/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0192 - accuracy: 0.9969\n",
      "Epoch 75/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0185 - accuracy: 0.9969\n",
      "Epoch 76/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0182 - accuracy: 0.9969\n",
      "Epoch 77/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0179 - accuracy: 0.9970\n",
      "Epoch 78/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0172 - accuracy: 0.9970\n",
      "Epoch 79/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0169 - accuracy: 0.9971\n",
      "Epoch 80/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0162 - accuracy: 0.9973\n",
      "Epoch 81/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0160 - accuracy: 0.9973\n",
      "Epoch 82/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0157 - accuracy: 0.9973\n",
      "Epoch 83/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0151 - accuracy: 0.9976\n",
      "Epoch 84/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0154 - accuracy: 0.9973\n",
      "Epoch 85/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0146 - accuracy: 0.9974\n",
      "Epoch 86/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0141 - accuracy: 0.9976\n",
      "Epoch 87/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0138 - accuracy: 0.9976\n",
      "Epoch 88/100\n",
      "162/162 [==============================] - 2s 12ms/step - loss: 0.0135 - accuracy: 0.9977\n",
      "Epoch 89/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0132 - accuracy: 0.9978\n",
      "Epoch 90/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0127 - accuracy: 0.9979\n",
      "Epoch 91/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0126 - accuracy: 0.9979\n",
      "Epoch 92/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0125 - accuracy: 0.9979\n",
      "Epoch 93/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0120 - accuracy: 0.9981\n",
      "Epoch 94/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0118 - accuracy: 0.9981\n",
      "Epoch 95/100\n",
      "162/162 [==============================] - 2s 12ms/step - loss: 0.0115 - accuracy: 0.9982\n",
      "Epoch 96/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0113 - accuracy: 0.9981\n",
      "Epoch 97/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0109 - accuracy: 0.9983\n",
      "Epoch 98/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0109 - accuracy: 0.9984\n",
      "Epoch 99/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0107 - accuracy: 0.9983\n",
      "Epoch 100/100\n",
      "162/162 [==============================] - 2s 11ms/step - loss: 0.0107 - accuracy: 0.9983\n",
      "Test loss: 0.021434804424643517\n",
      "Test accuracy: 0.9953638315200806\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Flatten(input_shape=input_shape),\n",
    "        layers.Dense(10, activation='relu'),\n",
    "        layers.Dense(10, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ]\n",
    ")\n",
    "model.summary()\n",
    "epochs = 100\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=keras.optimizers.Adam(learning_rate=1e-4), metrics=[\"accuracy\"])\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs)\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Clh_Ce4iX-cv"
   },
   "source": [
    "**Convolutional Neural Network**\n",
    "\n",
    "I tried to create a CNN model very similar to FCNN model. So, the number of parameters for this model is very close to FCNN (FCNN = 41146 , CNN = 41....)\n",
    "\n",
    "Also, it has 2 convolutional layers with relu activation, and in the last layer I have used a softmax layer to classify more than 2 classes.\n",
    "\n",
    "Other settings, like learning rate, optimizer, mini batch size, etc are similar to previous model. \n",
    "\n",
    "The results clearly shows that CNN is the best model among all the examined models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z8qQ0z9eXFva",
    "outputId": "ba33f01a-d22f-4525-e685-1eeeb1efd781"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 62, 62, 16)        160       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 31, 31, 16)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 29, 29, 31)        4495      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 14, 14, 31)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 6076)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 6)                 36462     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 41,117\n",
      "Trainable params: 41,117\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "162/162 [==============================] - 74s 454ms/step - loss: 1.0022 - accuracy: 0.7307\n",
      "Epoch 2/30\n",
      "162/162 [==============================] - 72s 441ms/step - loss: 0.2301 - accuracy: 0.9475\n",
      "Epoch 3/30\n",
      "162/162 [==============================] - 71s 438ms/step - loss: 0.1179 - accuracy: 0.9769\n",
      "Epoch 4/30\n",
      "162/162 [==============================] - 74s 457ms/step - loss: 0.0782 - accuracy: 0.9866\n",
      "Epoch 5/30\n",
      "162/162 [==============================] - 71s 440ms/step - loss: 0.0593 - accuracy: 0.9903\n",
      "Epoch 6/30\n",
      "162/162 [==============================] - 73s 451ms/step - loss: 0.0476 - accuracy: 0.9921\n",
      "Epoch 7/30\n",
      "162/162 [==============================] - 70s 431ms/step - loss: 0.0398 - accuracy: 0.9939\n",
      "Epoch 8/30\n",
      "162/162 [==============================] - 72s 443ms/step - loss: 0.0338 - accuracy: 0.9946\n",
      "Epoch 9/30\n",
      "162/162 [==============================] - 74s 459ms/step - loss: 0.0293 - accuracy: 0.9955\n",
      "Epoch 10/30\n",
      "162/162 [==============================] - 73s 450ms/step - loss: 0.0257 - accuracy: 0.9958\n",
      "Epoch 11/30\n",
      "162/162 [==============================] - 70s 433ms/step - loss: 0.0226 - accuracy: 0.9964\n",
      "Epoch 12/30\n",
      "162/162 [==============================] - 70s 435ms/step - loss: 0.0202 - accuracy: 0.9968\n",
      "Epoch 13/30\n",
      "162/162 [==============================] - 72s 442ms/step - loss: 0.0180 - accuracy: 0.9970\n",
      "Epoch 14/30\n",
      "162/162 [==============================] - 88s 545ms/step - loss: 0.0162 - accuracy: 0.9973\n",
      "Epoch 15/30\n",
      "162/162 [==============================] - 86s 528ms/step - loss: 0.0146 - accuracy: 0.9976\n",
      "Epoch 16/30\n",
      "162/162 [==============================] - 71s 438ms/step - loss: 0.0133 - accuracy: 0.9978\n",
      "Epoch 17/30\n",
      "162/162 [==============================] - 71s 439ms/step - loss: 0.0120 - accuracy: 0.9979\n",
      "Epoch 18/30\n",
      "162/162 [==============================] - 70s 431ms/step - loss: 0.0108 - accuracy: 0.9981\n",
      "Epoch 19/30\n",
      "162/162 [==============================] - 70s 430ms/step - loss: 0.0101 - accuracy: 0.9983\n",
      "Epoch 20/30\n",
      "162/162 [==============================] - 72s 444ms/step - loss: 0.0093 - accuracy: 0.9984\n",
      "Epoch 21/30\n",
      "162/162 [==============================] - 71s 436ms/step - loss: 0.0084 - accuracy: 0.9985\n",
      "Epoch 22/30\n",
      "162/162 [==============================] - 70s 431ms/step - loss: 0.0076 - accuracy: 0.9987\n",
      "Epoch 23/30\n",
      "162/162 [==============================] - 72s 445ms/step - loss: 0.0069 - accuracy: 0.9989\n",
      "Epoch 24/30\n",
      "162/162 [==============================] - 71s 436ms/step - loss: 0.0066 - accuracy: 0.9989\n",
      "Epoch 25/30\n",
      "162/162 [==============================] - 70s 433ms/step - loss: 0.0061 - accuracy: 0.9988\n",
      "Epoch 26/30\n",
      "162/162 [==============================] - 89s 548ms/step - loss: 0.0056 - accuracy: 0.9988\n",
      "Epoch 27/30\n",
      "162/162 [==============================] - 75s 463ms/step - loss: 0.0054 - accuracy: 0.9987\n",
      "Epoch 28/30\n",
      "162/162 [==============================] - 71s 436ms/step - loss: 0.0049 - accuracy: 0.9991\n",
      "Epoch 29/30\n",
      "162/162 [==============================] - 77s 476ms/step - loss: 0.0045 - accuracy: 0.9991\n",
      "Epoch 30/30\n",
      "162/162 [==============================] - 71s 440ms/step - loss: 0.0042 - accuracy: 0.9991\n",
      "Test loss: 0.008835149928927422\n",
      "Test accuracy: 0.9976819157600403\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(16, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(31, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "epochs = 30\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=keras.optimizers.Adam(learning_rate=1e-4), metrics=[\"accuracy\"])\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs)\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LI_wYVwwEHU"
   },
   "source": [
    "# Conclusion\n",
    "The artificial neural network and convolutional network both perform better than the baseline models. In this simple training cnn models (with acc= 0.9980)prforms slightly better than fully-connected neuram network (with acc=0.9966). However, I think the CNN is better suited for this dataset as it learns features from the images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NSdUmosJwEHV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "medicalMNISTproject.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
